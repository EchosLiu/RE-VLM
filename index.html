<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>RE-VLM: Event-Augmented Vision-Language Model for Scene Understanding</title>
  <meta name="description" content="RE-VLM is the first dual-stream vision-language model that jointly leverages RGB images and event streams for robust scene understanding across both normal and challenging conditions. CVPR 2026." />
  <meta name="keywords" content="RE-VLM, Event Camera, Vision-Language Model, Scene Understanding, CVPR 2026, Multimodal, Event-Augmented" />

  <meta property="og:title" content="RE-VLM: Event-Augmented Vision-Language Model" />
  <meta property="og:description" content="The first dual-stream VLM fusing RGB images and event streams for robust scene understanding. CVPR 2026." />
  <meta property="og:type" content="website" />

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400&family=Noto+Serif:ital,wght@0,400;0,500;0,600;0,700;1,400&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" />
  <link rel="stylesheet" href="static/css/style.css" />
</head>
<body>

  <!-- Hero Section -->
  <section class="hero">
    <div class="container">
      <div class="venue-badge">
        <span class="badge-icon"><i class="fa-solid fa-award"></i></span>
        <span>CVPR 2026</span>
      </div>
      <h1 class="title">
        <span class="title-highlight">RE-VLM</span>: Event-Augmented Vision-Language<br/>Model for Scene Understanding
      </h1>
      <div class="authors">
        <span class="author">Hanqing Liu<sup>1,*</sup></span>
        <span class="author">Mingjie Liu<sup>1,*</sup></span>
        <span class="author">Luoping Cui<sup>1</sup></span>
        <span class="author">Donghong Jiang<sup>1</sup></span>
        <span class="author">Endian Lin<sup>1</sup></span>
        <span class="author">Chuang Zhu<sup>1,&dagger;</sup></span>
      </div>
      <div class="affiliations">
        <span class="affiliation"><sup>1</sup>Beijing University of Posts and Telecommunications</span>
      </div>
      <div class="author-notes">
        <span>* Equal Contribution</span>
        <span>&dagger; Corresponding Author</span>
      </div>
      <div class="hero-links">
        <a href="#" class="hero-btn"><i class="fa-solid fa-file-pdf"></i> Paper</a>
        <a href="#" class="hero-btn"><i class="fa-brands fa-github"></i> Code</a>
        <a href="#" class="hero-btn"><i class="fa-solid fa-database"></i> Dataset</a>
        <a href="#bibtex" class="hero-btn"><i class="fa-solid fa-quote-left"></i> BibTeX</a>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section" id="abstract">
    <div class="container">
      <h2 class="section-title">Abstract</h2>
      <div class="abstract-content">
        <p>
          Conventional vision-language models (VLMs) struggle to interpret scenes captured under adverse conditions
          (e.g., low light, high dynamic range, or fast motion) because standard RGB images degrade in such environments.
          Event cameras provide a complementary modality: they asynchronously record per-pixel brightness changes with
          high temporal resolution and wide dynamic range, preserving motion cues where frames fail.
        </p>
        <p>
          We propose <strong>RE-VLM</strong>, the first dual-stream vision-language model that jointly leverages RGB images and event
          streams for robust scene understanding across both normal and challenging conditions. RE-VLM employs parallel
          RGB and event encoders together with a progressive training strategy that aligns heterogeneous visual features
          with language. To address the scarcity of RGB-Event-Text supervision, we further propose a <em>graph-driven pipeline</em>
          that converts synchronized RGB-Event streams into verifiable scene graphs, from which we synthesize captions
          and questionâ€“answer (QA) pairs.
        </p>
        <p>
          To develop and evaluate RE-VLM, we construct two datasets: <strong>PEOD-Chat</strong>, targeting illumination-challenged
          scenes, and <strong>RGBE-Chat</strong>, covering diverse scenarios. On captioning and VQA benchmarks, RE-VLM consistently
          outperforms state-of-the-art RGB-only and event-only models with comparable parameter counts, with particularly
          large gains under challenging conditions.
        </p>
      </div>
    </div>
  </section>

  <!-- Key Contributions -->
  <section class="section section-alt" id="contributions">
    <div class="container">
      <h2 class="section-title">Key Contributions</h2>
      <div class="contributions-grid">
        <div class="contribution-card">
          <div class="card-icon"><i class="fa-solid fa-layer-group"></i></div>
          <h3>Dual-Stream VLM</h3>
          <p>First VLM to fuse static RGB cues (texture/appearance) with dynamic event cues (motion/HDR), using a progressive training regimen for effective modality alignment.</p>
        </div>
        <div class="contribution-card">
          <div class="card-icon"><i class="fa-solid fa-diagram-project"></i></div>
          <h3>Graph-Driven Data Pipeline</h3>
          <p>A novel graph-driven data generation pipeline that synthesizes verifiable scene descriptions and QA pairs from synchronized RGB-Event inputs, addressing the RGB-Event-Text data scarcity.</p>
        </div>
        <div class="contribution-card">
          <div class="card-icon"><i class="fa-solid fa-chart-line"></i></div>
          <h3>State-of-the-Art Performance</h3>
          <p>RE-VLM outperforms strong RGB-only and event-only baselines (including models with 2&times; more parameters) with notable gains in caption fidelity under adverse conditions.</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section" id="method">
    <div class="container">
      <h2 class="section-title">Method</h2>

      <!-- Overview Figure -->
      <div class="method-subsection">
        <h3 class="subsection-title">Overall Framework</h3>
        <p class="method-text">
          RE-VLM consists of two main components: (1) a <em>graph-driven data generation pipeline</em> that converts
          synchronized RGB-Event streams into structured scene graphs for synthesizing training data, and (2) a
          <em>dual-stream model architecture</em> with parallel RGB and event encoders, a Spatio-Temporal Alignment
          Module (STAM), and an LLM decoder.
        </p>
        <div class="figure-block">
          <img src="static/images/fig2_overview.png" alt="RE-VLM overall framework: data generation pipeline, datasets, and model architecture" class="paper-figure" loading="lazy" />
          <p class="figure-caption">
            <strong>Figure 2.</strong> Construction of RE-VLM: data generation pipeline and model.
            <em>Left:</em> A graph-driven pipeline converts synchronized RGB frames and event streams into a fused graph, extracts verifiable scene facts, and synthesizes reliable caption and QA supervision.
            <em>Center:</em> Representative examples from PEOD-Chat and RGBE-Chat.
            <em>Right:</em> The RE-VLM model fuses static RGB appearance with dynamic event cues via STAM.
          </p>
        </div>
      </div>

      <!-- Data Pipeline -->
      <div class="method-subsection">
        <h3 class="subsection-title">Graph-Driven Data Generation</h3>
        <p class="method-text">
          To address the scarcity of RGB-Event-Text training data, we introduce a degradation-adaptive data generation pipeline.
          A scene graph serves as a verifiable intermediate representation, explicitly encoding degradation cues to guide modality weighting.
          The pipeline converts synchronized RGB and event streams into graph-structured scene facts and then synthesizes caption and VQA supervision.
        </p>
        <div class="figure-block">
          <img src="static/images/fig3_data_pipeline.png" alt="Data generation pipeline: Event/RGB graph generation, fusion, and QA synthesis" class="paper-figure" loading="lazy" />
          <p class="figure-caption">
            <strong>Figure 3.</strong> Data generation pipeline overview.
            From reconstructed event frames and RGB images, two modality-specific graphs are constructed.
            A degradation-aware fusion then merges them into a single RGB-Event graph (nodes: entities, edges: relations).
            Finally, captions and VQA items are synthesized from the fused graph.
          </p>
        </div>
      </div>

      <!-- Model Architecture -->
      <div class="method-subsection">
        <h3 class="subsection-title">Model Architecture</h3>
        <p class="method-text">
          RE-VLM leverages the event stream's high temporal resolution and extended dynamic range together with
          the RGB image's static appearance. The architecture comprises an RGB vision encoder, an event vision encoder,
          a lightweight Spatio-Temporal Alignment Module (STAM, training-time only), two modality adapters, and an LLM decoder.
        </p>
        <div class="figure-block">
          <img src="static/images/fig4_architecture.png" alt="RE-VLM model architecture with STAM, dual encoders, and LLM" class="paper-figure" loading="lazy" />
          <p class="figure-caption">
            <strong>Figure 4.</strong> RE-VLM model architecture. Synchronized RGB and event streams are encoded by parallel vision encoders.
            During training, a Spatio-Temporal Alignment Module (STAM) provides alignment signals and a relation loss.
            During inference, event features are projected (event adapter) and, together with RGB tokens (RGB adapter), are fed to the LLM.
          </p>
        </div>
      </div>

      <!-- Training Pipeline -->
      <div class="method-subsection">
        <h3 class="subsection-title">Progressive Training Strategy</h3>
        <p class="method-text">
          RE-VLM adopts a concise three-stage curriculum that first grounds the event stream to language, then aligns
          it with the RGB representation via STAM, and finally performs lightweight instruction tuning on the LLM.
        </p>
        <div class="figure-block">
          <img src="static/images/fig5_training.png" alt="Three-stage training pipeline: Event-Language alignment, Event-RGB alignment, Instruction tuning" class="paper-figure" loading="lazy" />
          <p class="figure-caption">
            <strong>Figure 5.</strong> Training pipeline. Three compact stages:
            (1) Initial event&ndash;language alignment,
            (2) Align the event and RGB modalities with STAM,
            (3) End-to-end instruction tuning.
          </p>
        </div>
      </div>

    </div>
  </section>

  <!-- Datasets -->
  <section class="section section-alt" id="datasets">
    <div class="container">
      <h2 class="section-title">Datasets</h2>
      <p class="method-text center-text">
        We construct two multimodal dialogue benchmarks tailored for challenging illumination and general scenes.
      </p>
      <div class="dataset-cards">
        <div class="dataset-card">
          <div class="dataset-header peod-header">
            <h3>PEOD-Chat</h3>
            <span class="dataset-size">11K samples</span>
          </div>
          <div class="dataset-body">
            <p>Targeting <strong>illumination-challenged</strong> scenes (low light, overexposure, motion blur).</p>
            <ul>
              <li>Source: PEOD dataset</li>
              <li>Test split: 1,750 samples</li>
              <li>60% adverse imaging, 40% normal illumination</li>
              <li>Human-corrected text supervision</li>
            </ul>
          </div>
        </div>
        <div class="dataset-card">
          <div class="dataset-header rgbe-header">
            <h3>RGBE-Chat</h3>
            <span class="dataset-size">113.7K samples</span>
          </div>
          <div class="dataset-body">
            <p>Covering <strong>diverse general scenarios</strong> from multiple event camera datasets.</p>
            <ul>
              <li>Sources: RGBE-ImageNet (60K), DSEC (12K), DDD17 (1.7K), RGBE-SEG (15K), MVSEC (11K), M3ED (14K)</li>
              <li>Test split: 2,047 samples</li>
              <li>In-the-wild scenes with strict sequence-level split</li>
            </ul>
          </div>
        </div>
      </div>

      <div class="table-block">
        <p class="table-title">Table 1. Manual QA corrections on PEOD samples</p>
        <p class="table-subtitle">Human-audited correction rate comparing an RGB-only generation baseline with our method; lower is better.</p>
        <table class="results-table compact-table">
          <thead>
            <tr>
              <th>Method</th>
              <th>Correction Rate</th>
              <th>Correction Count</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>RGB VLM [EventGPT]</td>
              <td>54.2%</td>
              <td>463</td>
            </tr>
            <tr class="highlight-row">
              <td><strong>Ours</strong></td>
              <td><strong>18.1%</strong></td>
              <td><strong>155</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </section>

  <!-- Results -->
  <section class="section" id="results">
    <div class="container">
      <h2 class="section-title">Results</h2>

      <div class="results-subsection">
        <h3 class="subsection-title">Comparison with State-of-the-Art</h3>
        <p class="method-text">
          We report LLM-judge scores for Caption and VQA on both benchmarks. RE-VLM attains the best performance across
          all metrics, with particularly large gains on illumination-challenged PEOD-Chat.
        </p>

        <div class="table-block">
          <p class="table-title">Table 3. Results on PEOD-Chat and RGBE-Chat</p>
          <p class="table-subtitle">LLM-judge scores for Caption (CI, DO, CU) and VQA (Ave, Acc). (*) denotes fine-tuned variants.</p>
          <div class="table-wrapper">
            <table class="results-table main-table">
              <thead>
                <tr>
                  <th rowspan="2">Input</th>
                  <th rowspan="2">Model</th>
                  <th rowspan="2">Params</th>
                  <th colspan="4" class="group-header peod-group">PEOD-Chat</th>
                  <th colspan="4" class="group-header rgbe-group">RGBE-Chat</th>
                </tr>
                <tr>
                  <th>CI</th>
                  <th>DO</th>
                  <th>CU</th>
                  <th>Acc</th>
                  <th>CI</th>
                  <th>DO</th>
                  <th>CU</th>
                  <th>Acc</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td rowspan="5" class="input-cell">RGB-only</td>
                  <td>Qwen2.5-VL</td>
                  <td>3B</td>
                  <td>2.47</td><td>2.03</td><td>3.04</td><td>0.52</td>
                  <td>3.34</td><td>2.70</td><td>3.64</td><td>0.66</td>
                </tr>
                <tr>
                  <td>InternVL2</td>
                  <td>4B</td>
                  <td>3.09</td><td>2.38</td><td>3.68</td><td>0.49</td>
                  <td>3.47</td><td>2.84</td><td>3.91</td><td>0.68</td>
                </tr>
                <tr>
                  <td>DeepSeek2-VL</td>
                  <td>7B</td>
                  <td>3.25</td><td>2.42</td><td>3.73</td><td>0.50</td>
                  <td>3.63</td><td>2.89</td><td>4.11</td><td>0.52</td>
                </tr>
                <tr>
                  <td>LLaVA-1.5</td>
                  <td>7B</td>
                  <td>2.71</td><td>2.05</td><td>3.03</td><td>0.54</td>
                  <td>3.12</td><td>2.31</td><td>3.60</td><td>0.62</td>
                </tr>
                <tr>
                  <td>Qwen2.5-VL*</td>
                  <td>3B</td>
                  <td>3.23</td><td>2.74</td><td>3.51</td><td>0.55</td>
                  <td>3.91</td><td>3.41</td><td>4.27</td><td>0.65</td>
                </tr>
                <tr class="separator-row">
                  <td rowspan="2" class="input-cell">Event-only</td>
                  <td>EventGPT</td>
                  <td>7B</td>
                  <td>2.51</td><td>2.06</td><td>2.65</td><td>0.40</td>
                  <td>2.82</td><td>2.34</td><td>3.08</td><td>0.39</td>
                </tr>
                <tr>
                  <td>Qwen2.5-VL*</td>
                  <td>3B</td>
                  <td>2.74</td><td>2.48</td><td>2.97</td><td>0.45</td>
                  <td>2.79</td><td>2.57</td><td>3.16</td><td>0.58</td>
                </tr>
                <tr class="highlight-row separator-row">
                  <td class="input-cell">RGB+Event</td>
                  <td><strong>RE-VLM</strong></td>
                  <td>4B</td>
                  <td><strong>3.68</strong></td><td><strong>3.12</strong></td><td><strong>3.95</strong></td><td><strong>0.63</strong></td>
                  <td><strong>4.03</strong></td><td><strong>3.50</strong></td><td><strong>4.34</strong></td><td><strong>0.75</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

      <div class="results-subsection">
        <h3 class="subsection-title">Ablation Study</h3>
        <div class="ablation-tables">
          <div class="table-block">
            <p class="table-title">Table 4. Ablation on PEOD-Chat</p>
            <p class="table-subtitle">Impact of input modality and STAM module.</p>
            <table class="results-table compact-table">
              <thead>
                <tr>
                  <th>Input</th>
                  <th>STAM</th>
                  <th>CI</th>
                  <th>DO</th>
                  <th>CU</th>
                  <th>Acc</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>RGB-only</td><td>&mdash;</td><td>3.05</td><td>2.51</td><td>3.32</td><td>0.57</td></tr>
                <tr><td>Event-only</td><td>&mdash;</td><td>2.82</td><td>2.57</td><td>3.09</td><td>0.48</td></tr>
                <tr><td>RGB+Event</td><td>&#10007;</td><td>3.62</td><td>3.08</td><td>3.91</td><td>0.61</td></tr>
                <tr class="highlight-row"><td>RGB+Event</td><td>&#10003;</td><td><strong>3.68</strong></td><td><strong>3.12</strong></td><td><strong>3.95</strong></td><td><strong>0.63</strong></td></tr>
              </tbody>
            </table>
          </div>
          <div class="table-block">
            <p class="table-title">Table 5. Ablation on RGBE-Chat</p>
            <p class="table-subtitle">Impact of input modality and STAM module.</p>
            <table class="results-table compact-table">
              <thead>
                <tr>
                  <th>Input</th>
                  <th>STAM</th>
                  <th>CI</th>
                  <th>DO</th>
                  <th>CU</th>
                  <th>Acc</th>
                </tr>
              </thead>
              <tbody>
                <tr><td>RGB-only</td><td>&mdash;</td><td>3.97</td><td>3.46</td><td>4.32</td><td>0.73</td></tr>
                <tr><td>Event-only</td><td>&mdash;</td><td>2.49</td><td>2.48</td><td>2.82</td><td>0.57</td></tr>
                <tr><td>RGB+Event</td><td>&#10007;</td><td>4.01</td><td>3.47</td><td>4.35</td><td>0.74</td></tr>
                <tr class="highlight-row"><td>RGB+Event</td><td>&#10003;</td><td><strong>4.03</strong></td><td><strong>3.50</strong></td><td><strong>4.34</strong></td><td><strong>0.75</strong></td></tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

      <div class="results-subsection">
        <h3 class="subsection-title">Cross-Validation with Open-Source Judge</h3>
        <div class="table-block">
          <p class="table-title">Table 6. Evaluation using Qwen3-Omni-30B as judge</p>
          <p class="table-subtitle">Consistent trends with GPT-3.5-Turbo evaluation across all metrics.</p>
          <table class="results-table compact-table">
            <thead>
              <tr>
                <th>Task</th>
                <th>Metric</th>
                <th>Qwen2.5VL</th>
                <th>EventGPT</th>
                <th>RE-VLM</th>
              </tr>
            </thead>
            <tbody>
              <tr><td rowspan="3" class="input-cell">Caption</td><td>CI</td><td>2.17</td><td>1.99</td><td class="best-val"><strong>3.29</strong></td></tr>
              <tr><td>DO</td><td>1.82</td><td>1.89</td><td class="best-val"><strong>3.45</strong></td></tr>
              <tr><td>CU</td><td>2.71</td><td>2.50</td><td class="best-val"><strong>3.85</strong></td></tr>
              <tr class="separator-row"><td rowspan="2" class="input-cell">VQA</td><td>Ave</td><td>2.78</td><td>2.38</td><td class="best-val"><strong>3.43</strong></td></tr>
              <tr><td>Acc</td><td>0.48</td><td>0.40</td><td class="best-val"><strong>0.62</strong></td></tr>
            </tbody>
          </table>
        </div>
      </div>

    </div>
  </section>

  <!-- Qualitative Results -->
  <section class="section section-alt" id="qualitative">
    <div class="container">
      <h2 class="section-title">Qualitative Results</h2>
      <p class="method-text center-text">
        Qualitative VQA comparison in an overexposed traffic scene. RGB-only and event-only baselines miss the city bus or
        fail to capture the color, while RE-VLM correctly identifies both.
      </p>
      <div class="figure-block">
        <img src="static/images/fig6_qualitative.png" alt="Qualitative VQA comparison showing RE-VLM correctly identifies city bus and vehicle color" class="paper-figure" loading="lazy" />
        <p class="figure-caption">
          <strong>Figure 6.</strong> Qualitative VQA comparison in an overexposed traffic scene.
          RGB-only VLM (Qwen2.5-VL) fails to detect the city bus under overexposure.
          Event-only VLM (EventGPT) cannot determine appearance attributes like color.
          Our <strong>RE-VLM</strong> correctly identifies both the city bus and the white vehicle color by fusing complementary RGB and event cues.
        </p>
      </div>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section" id="bibtex">
    <div class="container">
      <h2 class="section-title">BibTeX</h2>
      <p class="method-text center-text">
        If you find our work useful, please consider citing:
      </p>
      <div class="bibtex-block">
        <button class="copy-btn" id="copyBibtex" title="Copy to clipboard">
          <i class="fa-regular fa-copy"></i> Copy
        </button>
        <pre><code>@inproceedings{revlm2026,
  title     = {RE-VLM: Event-Augmented Vision-Language Model
               for Scene Understanding},
  author    = {Liu, Hanqing and Liu, Mingjie and Cui, Luoping and Jiang, Donghong and Lin, Endian and Zhu, Chuang},
  booktitle = {Proceedings of the IEEE/CVF Conference on
               Computer Vision and Pattern Recognition (CVPR)},
  year      = {2026}
}</code></pre>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <p>
        This page template is inspired by
        <a href="https://nerfies.github.io" target="_blank">Nerfies</a> and
        <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
      </p>
    </div>
  </footer>

  <script src="static/js/main.js"></script>
</body>
</html>
