# RE-VLM: Event-Augmented Vision-Language Model for Scene Understanding

**CVPR 2026**

> RE-VLM is the first dual-stream vision-language model that jointly leverages RGB images and event streams for robust scene understanding across both normal and challenging conditions.

```

### Deployment

This page is designed to be deployed via **GitHub Pages**. To enable:

### Customization

- **Authors & Affiliations**: Edit the hero section in `index.html`
- **Figures**: Replace placeholder content with actual paper figures in `static/images/`
- **BibTeX**: Update the citation block in `index.html`
- **Links**: Update Paper, Code, Dataset, Video links in the hero section

## Project Structure

```
├── index.html              # Main page
├── static/
│   ├── css/
│   │   └── style.css       # Styling
│   ├── js/
│   │   └── main.js         # Interactions (copy BibTeX, scroll animations)
│   ├── images/             # Paper figures (add your own)
│   └── videos/             # Demo videos (optional)
├── README.md
└── LICENSE
```

## Citation

```bibtex

```

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.
